# ðŸŒŠ Streams in Node.js â€“ Deep Dive (Zero-Copy Power)

> **Goal:** Samajhna ki Streams **kyon Node.js ka sabse powerful feature** hain,
> kaise ye memory bachate hain, aur production systems me **large data** kaise handle hota hai.

---

## ðŸ§  What are Streams?

**Streams** = data ko **chunk-by-chunk** process karna instead of ek saath load karna.

ðŸ“Œ Simple analogy:

> **Bucket se pani peene ke bajaye pipe se pani peena** ðŸš°

---

## ðŸ§© Why Streams Matter (Real Pain)

âŒ Without streams:

```js
const data = fs.readFileSync('1GB.mp4'); // ðŸ’¥ RAM dead
```

âœ… With streams:

```js
fs.createReadStream('1GB.mp4').pipe(res);
```

ðŸ”¥ Same result, **90% less memory**

---

## ðŸ“¦ Types of Streams

### 1ï¸âƒ£ Readable Stream ðŸ“¥

* Data **consume** karta hai
* Examples:

  * fs.createReadStream
  * HTTP request

---

### 2ï¸âƒ£ Writable Stream ðŸ“¤

* Data **write** karta hai
* Examples:

  * fs.createWriteStream
  * HTTP response

---

### 3ï¸âƒ£ Duplex Stream ðŸ”„

* Read + Write dono
* Example:

  * TCP socket

---

### 4ï¸âƒ£ Transform Stream ðŸ” (ðŸ”¥ Most Powerful)

* Input â†’ process â†’ output
* Examples:

  * gzip
  * crypto

---

## ðŸ”— Stream Pipeline (Mental Model)

```
Readable â†’ Transform â†’ Writable
```

Pipeline = **assembly line** ðŸ­

---

## ðŸ§ª Basic Stream Example

```js
const fs = require('fs');

const read = fs.createReadStream('input.txt');
const write = fs.createWriteStream('output.txt');

read.pipe(write);
```

ðŸ“Œ Automatic backpressure handling

---

## ðŸš¦ Backpressure (Very Important âš ï¸)

> Jab writer slow ho aur reader fast ho

Streams automatically:

* Pause reader
* Resume when ready

ðŸ”¥ Ye hi reason hai streams **safe** hote hain

---

## ðŸ”„ Stream Modes

### ðŸ§µ Flowing Mode

* Data auto flow hota hai
* `data` event

### ðŸ§ Paused Mode

* Manual read
* `.read()`

---

## ðŸ§  HighWaterMark

```js
fs.createReadStream('file', {
  highWaterMark: 64 * 1024
});
```

ðŸ“Œ Chunk size control

Trade-off:

* Small chunk â†’ more overhead
* Big chunk â†’ more memory

---

## ðŸ” Transform Stream Example

```js
const { Transform } = require('stream');

const upper = new Transform({
  transform(chunk, enc, cb) {
    cb(null, chunk.toString().toUpperCase());
  }
});

process.stdin.pipe(upper).pipe(process.stdout);
```

ðŸ”¥ Real-time processing

---

## ðŸ“¦ Streams vs Buffers

| Buffers       | Streams      |
| ------------- | ------------ |
| Load all data | Chunked data |
| High memory   | Low memory   |
| Simple        | Scalable     |

---

## ðŸŒ Streams in HTTP (Production Use)

```js
http.createServer((req, res) => {
  fs.createReadStream('video.mp4').pipe(res);
});
```

ðŸŽ¥ Video streaming, file download

---

## ðŸ§  Streams & Memory Management

* Minimal heap usage
* No large buffers
* GC pressure kam

ðŸ‘‰ Perfect with **memory-management.md**

---

## ðŸš¨ Common Mistakes

âŒ readFile for large files
âŒ Ignoring errors
âŒ Not closing streams

```js
read.on('error', console.error);
```

---

## âš¡ Best Practices (Production Gold)

âœ… Always use `pipeline()`

```js
const { pipeline } = require('stream');
```

âœ… Handle errors
âœ… Tune highWaterMark

---

## ðŸ§  Interview Gold âœ¨

> **Streams enable backpressure handling**

> **Transform streams allow real-time data processing**

> **Streams reduce memory footprint drastically**

---

## ðŸ“Œ TL;DR

* Streams = chunk-based processing
* Low memory, high scalability
* Backpressure is automatic
* Essential for large data
